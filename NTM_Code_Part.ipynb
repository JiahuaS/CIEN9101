{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35982d88",
   "metadata": {},
   "source": [
    "# <center> Nontrivial Transform Method in Physics-Informed Deep Learning Discovering Partial Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7b1f7",
   "metadata": {},
   "source": [
    "### <center>Jiahua Song\n",
    "### <center>Department of Applied Physics and Applied Mathematics\n",
    "### <center>Columbia University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad5dff",
   "metadata": {},
   "source": [
    "This is the code part of the paper for realization and some tests on Nontrivial Transform Method (NTM). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68801636",
   "metadata": {},
   "source": [
    "We proceed some simple examples in code realizations on the paper sampling data from $u_t=u_{xx}$ and some parabolic PDE using SGA-PDE, PEQL, and finally test the NTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e399b0",
   "metadata": {},
   "source": [
    "# SGA-PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc639ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/jiahuasong/anaconda3/envs/tensorflow/lib/python3.10/site-packages (1.23.5)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting deap\n",
      "  Downloading deap-1.4.1-cp310-cp310-macosx_11_0_x86_64.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.7/104.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19 (from sympy)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, deap\n",
      "Successfully installed deap-1.4.1 mpmath-1.3.0 sympy-1.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy sympy deap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086193a2",
   "metadata": {},
   "source": [
    "### Import packages and Setup the symbolic Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13728fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiahuasong/anaconda3/envs/tensorflow/lib/python3.10/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/Users/jiahuasong/anaconda3/envs/tensorflow/lib/python3.10/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg      \tstd      \tmin      \tmax     \n",
      "0  \t300   \t0.0369561\t0.0421625\t3.954e-07\t0.189365\n",
      "1  \t172   \t0.0281516\t0.108275 \t3.954e-07\t1.32777 \n",
      "2  \t182   \t0.0207574\t0.0601275\t8.65505e-08\t0.450854\n",
      "3  \t180   \t0.0216463\t0.0805255\t5.84909e-08\t0.805689\n",
      "4  \t168   \t0.0190509\t0.073666 \t5.84909e-08\t0.717915\n",
      "5  \t176   \t0.0153941\t0.0562085\t5.84909e-08\t0.38138 \n",
      "6  \t181   \t0.0193566\t0.0891006\t4.61453e-08\t1.18802 \n",
      "7  \t172   \t0.008082 \t0.0531787\t2.05227e-08\t0.839691\n",
      "8  \t174   \t0.0141261\t0.0709506\t1.84962e-08\t0.784342\n",
      "9  \t180   \t0.00670486\t0.0518681\t1.84962e-08\t0.805387\n",
      "10 \t188   \t0.0104154 \t0.0823391\t1.31597e-08\t1.15929 \n",
      "11 \t185   \t0.0120352 \t0.0768804\t1.31597e-08\t1.03118 \n",
      "12 \t198   \t0.010148  \t0.0609352\t1.32068e-09\t0.571683\n",
      "13 \t166   \t0.0152806 \t0.108659 \t1.94017e-10\t1.45919 \n",
      "14 \t168   \t0.0116387 \t0.0955929\t9.86901e-14\t1.38852 \n",
      "15 \t155   \t0.00519587\t0.0668311\t9.86901e-14\t1.13567 \n",
      "16 \t183   \t0.0086077 \t0.0607487\t9.86901e-14\t0.841683\n",
      "17 \t172   \t0.00545069\t0.0479263\t9.86901e-14\t0.748678\n",
      "18 \t169   \t0.00191766\t0.0264744\t1.07905e-14\t0.446519\n",
      "19 \t175   \t0.0190472 \t0.141459 \t1.07905e-14\t1.78057 \n",
      "20 \t170   \t0.00688873\t0.0996438\t1.07905e-14\t1.71674 \n",
      "21 \t180   \t0.00896116\t0.0649354\t1.07905e-14\t0.610066\n",
      "22 \t173   \t0.0149093 \t0.121062 \t2.12308e-15\t1.75507 \n",
      "23 \t195   \t0.0121506 \t0.123094 \t1.88167e-15\t1.92813 \n",
      "24 \t181   \t0.00145931\t0.0130743\t5.48475e-17\t0.180315\n",
      "25 \t174   \t0.00584725\t0.0383887\t1.94227e-17\t0.433623\n",
      "26 \t187   \t0.0100733 \t0.090511 \t4.21376e-22\t1.4349  \n",
      "27 \t188   \t0.00695721\t0.0513863\t4.21376e-22\t0.620315\n",
      "28 \t168   \t0.005469  \t0.0495963\t4.21376e-22\t0.673435\n",
      "29 \t173   \t0.00727496\t0.0522727\t2.08083e-20\t0.669969\n",
      "30 \t188   \t0.00869172\t0.0982076\t2.08083e-20\t1.45762 \n",
      "31 \t164   \t0.00159017\t0.0133172\t2.08083e-20\t0.151742\n",
      "32 \t166   \t0.0113133 \t0.0707845\t7.08036e-21\t0.72819 \n",
      "33 \t172   \t0.0122976 \t0.0856118\t7.96511e-23\t0.870307\n",
      "34 \t172   \t0.00495691\t0.0526922\t2.08083e-20\t0.810794\n",
      "35 \t200   \t0.00340941\t0.03108  \t8.78339e-22\t0.47227 \n",
      "36 \t179   \t0.00620266\t0.036984 \t8.78339e-22\t0.356685\n",
      "37 \t179   \t0.0101852 \t0.0819995\t4.05415e-22\t1.16322 \n",
      "38 \t162   \t0.00939116\t0.120305 \t1.19167e-22\t2.01352 \n",
      "39 \t183   \t0.0168098 \t0.129457 \t1.05349e-24\t1.90835 \n",
      "40 \t178   \t0.00993981\t0.0917478\t4.92822e-25\t1.09409 \n",
      "Best individual is:  [0.651450460715373, 0.34854953928311183]\n",
      "Best fitness is:  (4.92821505378649e-25,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "from deap import creator, base, tools, algorithms\n",
    "\n",
    "# Define symbols\n",
    "t, x = sp.symbols('t x')\n",
    "u = sp.Function('u')(t, x)\n",
    "\n",
    "# Exact Solution\n",
    "solution = sp.exp(-t) * sp.sin(x)\n",
    "u_t = solution.diff(t)\n",
    "u_xx = solution.diff(x, x)\n",
    "\n",
    "# Generate the data\n",
    "x_vals = np.linspace(0, np.pi, 100)\n",
    "t_vals = np.linspace(0, 1, 100)\n",
    "X, T = np.meshgrid(x_vals, t_vals)\n",
    "U_t = sp.lambdify((x, t), u_t)(X, T)\n",
    "U_xx = sp.lambdify((x, t), u_xx)(X, T)\n",
    "\n",
    "# Define the individual\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", np.random.rand)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=2)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(individual):\n",
    "    a, b = individual\n",
    "    prediction = a * U_t + b * U_xx\n",
    "    error = np.mean((prediction - U_t)**2)\n",
    "    return (error,)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Genetic Algorithm\n",
    "pop = toolbox.population(n=300)\n",
    "hof = tools.HallOfFame(1)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "result = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=40, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "# SGA-PDE optimal PDE\n",
    "print(\"Best individual is: \", hof[0])\n",
    "print(\"Best fitness is: \", hof[0].fitness.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6c60e",
   "metadata": {},
   "source": [
    "Thus, we get $0.65u_t+0.35u_{xx}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7602be7",
   "metadata": {},
   "source": [
    "# PEQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9052524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.571855545043945\n",
      "Epoch 100, Loss: 0.06058093160390854\n",
      "Epoch 200, Loss: 0.04522167891263962\n",
      "Epoch 300, Loss: 0.03532188758254051\n",
      "Epoch 400, Loss: 0.02228459157049656\n",
      "Epoch 500, Loss: 0.010847758501768112\n",
      "Epoch 600, Loss: 0.00551626505330205\n",
      "Epoch 700, Loss: 0.003123731119558215\n",
      "Epoch 800, Loss: 0.001861655036918819\n",
      "Epoch 900, Loss: 0.0011529753683134913\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m t_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t_data, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     73\u001b[0m x_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x_data, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 74\u001b[0m U_t, U_xx \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_pde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m pde_residuals \u001b[38;5;241m=\u001b[39m U_t \u001b[38;5;241m-\u001b[39m U_xx\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPDE Residuals Mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pde_residuals\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn[39], line 67\u001b[0m, in \u001b[0;36mevaluate_pde\u001b[0;34m(model, t, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m     U_x \u001b[38;5;241m=\u001b[39m tape2\u001b[38;5;241m.\u001b[39mgradient(U, x)\n\u001b[1;32m     66\u001b[0m     tape3\u001b[38;5;241m.\u001b[39mwatch(X)\n\u001b[0;32m---> 67\u001b[0m     U_xx \u001b[38;5;241m=\u001b[39m \u001b[43mtape3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m U_t, U_xx\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1021\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[1;32m   1010\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtape inside its context is significantly less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient in order to compute higher order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1021\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `target` should be a list or nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiated, but received None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m flat_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(target):\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(num_points, time_max, x_max):\n",
    "    t = np.linspace(0, time_max, num_points)\n",
    "    x = np.linspace(0, x_max, num_points)\n",
    "    T, X = np.meshgrid(t, x)\n",
    "    U = np.exp(-T) * np.sin(X)\n",
    "    return T.flatten(), X.flatten(), U.flatten()\n",
    "\n",
    "# PEQL\n",
    "class PEQLModel(Model):\n",
    "    def __init__(self):\n",
    "        super(PEQLModel, self).__init__()\n",
    "        self.dense1 = Dense(20, activation=\"tanh\")\n",
    "        self.dense2 = Dense(20, activation=\"tanh\")\n",
    "        self.out = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Train\n",
    "t_data, x_data, u_data = generate_data(100, 1, 2*np.pi)\n",
    "train_data = np.stack([t_data, x_data], axis=1)\n",
    "\n",
    "\n",
    "model = PEQLModel()\n",
    "\n",
    "def train_step(model, inputs, outputs, optimizer):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(inputs)\n",
    "        predicted = model(inputs)\n",
    "        loss = tf.reduce_mean((predicted - outputs)**2)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.01)\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    inputs = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "    outputs = tf.convert_to_tensor(u_data.reshape(-1, 1), dtype=tf.float32)\n",
    "    loss = train_step(model, inputs, outputs, optimizer)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Evaluate the discovered PDE\n",
    "def evaluate_pde(model, t, x):\n",
    "    X = tf.stack([t, x], axis=1)\n",
    "    with tf.GradientTape(persistent=True) as tape1, tf.GradientTape(persistent=True) as tape2, tf.GradientTape(persistent=True) as tape3:\n",
    "        tape1.watch(X)\n",
    "        U = model(X)\n",
    "        U_t = tape1.gradient(U, t)\n",
    "        \n",
    "        tape2.watch(X)\n",
    "        U_x = tape2.gradient(U, x)\n",
    "        \n",
    "        tape3.watch(X)\n",
    "        U_xx = tape3.gradient(U_x, x)\n",
    "        \n",
    "    return U_t, U_xx\n",
    "\n",
    "# Check the PDE residuals\n",
    "t_test = tf.convert_to_tensor(t_data, dtype=tf.float32)\n",
    "x_test = tf.convert_to_tensor(x_data, dtype=tf.float32)\n",
    "U_t, U_xx = evaluate_pde(model, t_test, x_test)\n",
    "pde_residuals = U_t - U_xx\n",
    "print(\"PDE Residuals Mean:\", pde_residuals.numpy().mean())\n",
    "print(\"PDE Residuals Std:\", pde_residuals.numpy().std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cb7ba",
   "metadata": {},
   "source": [
    "This code illustrates how the PEQL method can be used to discover the underlying PDE from synthetic data. We have the final loss $0.0011529753683134913$ which is pretty small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e88ecdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m         U_xx \u001b[38;5;241m=\u001b[39m tape3\u001b[38;5;241m.\u001b[39mgradient(U_x, x)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U, U_t, U_x, U_xx\n\u001b[0;32m---> 17\u001b[0m U, U_t, U_x, U_xx \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_pde_coefficients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m coeff_u_t \u001b[38;5;241m=\u001b[39m U_t \u001b[38;5;241m/\u001b[39m U\n\u001b[1;32m     21\u001b[0m coeff_u_xx \u001b[38;5;241m=\u001b[39m U_xx \u001b[38;5;241m/\u001b[39m U\n",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m, in \u001b[0;36mevaluate_pde_coefficients\u001b[0;34m(model, t, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     U_x \u001b[38;5;241m=\u001b[39m tape2\u001b[38;5;241m.\u001b[39mgradient(U, x)\n\u001b[1;32m     12\u001b[0m     tape3\u001b[38;5;241m.\u001b[39mwatch(X)\n\u001b[0;32m---> 13\u001b[0m     U_xx \u001b[38;5;241m=\u001b[39m \u001b[43mtape3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m U, U_t, U_x, U_xx\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1021\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[1;32m   1010\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtape inside its context is significantly less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient in order to compute higher order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1021\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `target` should be a list or nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiated, but received None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m flat_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(target):\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "# Coefficients of PDE gotten from PEQL\n",
    "def evaluate_pde_coefficients(model, t, x):\n",
    "    X = tf.stack([t, x], axis=1)\n",
    "    with tf.GradientTape(persistent=True) as tape1, tf.GradientTape(persistent=True) as tape2, tf.GradientTape(persistent=True) as tape3:\n",
    "        tape1.watch(X)\n",
    "        U = model(X)\n",
    "        U_t = tape1.gradient(U, t)\n",
    "        \n",
    "        tape2.watch(X)\n",
    "        U_x = tape2.gradient(U, x)\n",
    "        \n",
    "        tape3.watch(X)\n",
    "        U_xx = tape3.gradient(U_x, x)\n",
    "    \n",
    "    return U, U_t, U_x, U_xx\n",
    "\n",
    "U, U_t, U_x, U_xx = evaluate_pde_coefficients(model, t_test, x_test)\n",
    "\n",
    "\n",
    "coeff_u_t = U_t / U\n",
    "coeff_u_xx = U_xx / U\n",
    "\n",
    "print(\"Coefficient of u_t:\", coeff_u_t.numpy().mean())\n",
    "print(\"Coefficient of u_xx:\", coeff_u_xx.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc9154",
   "metadata": {},
   "source": [
    "# NTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed411fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.1848279982805252\n",
      "Epoch 100, Loss: 0.03418554738163948\n",
      "Epoch 200, Loss: 0.01264372281730175\n",
      "Epoch 300, Loss: 0.0013992481399327517\n",
      "Epoch 400, Loss: 0.00044998625526204705\n",
      "Epoch 500, Loss: 0.0014116890961304307\n",
      "Epoch 600, Loss: 0.00012380891712382436\n",
      "Epoch 700, Loss: 7.598046067869291e-05\n",
      "Epoch 800, Loss: 6.904051406309009e-05\n",
      "Epoch 900, Loss: 4.7597408411093056e-05\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m t_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t_data, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     73\u001b[0m x_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x_data, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 74\u001b[0m U_t, U_xx \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_pde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m pde_residuals \u001b[38;5;241m=\u001b[39m U_t \u001b[38;5;241m-\u001b[39m U_xx\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPDE Residuals Mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pde_residuals\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn[41], line 67\u001b[0m, in \u001b[0;36mevaluate_pde\u001b[0;34m(model, t, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape3:\n\u001b[1;32m     66\u001b[0m     tape3\u001b[38;5;241m.\u001b[39mwatch(X)\n\u001b[0;32m---> 67\u001b[0m     U_xx \u001b[38;5;241m=\u001b[39m \u001b[43mtape3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m U_t, U_xx\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1021\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[1;32m   1010\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtape inside its context is significantly less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient in order to compute higher order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1021\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `target` should be a list or nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiated, but received None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m flat_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(target):\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate synthetic data \n",
    "def generate_data(num_points, time_max, x_max):\n",
    "    t = np.linspace(0, time_max, num_points)\n",
    "    x = np.linspace(0, x_max, num_points)\n",
    "    T, X = np.meshgrid(t, x)\n",
    "    U = np.exp(-T) * np.sin(X)\n",
    "    return T.flatten(), X.flatten(), U.flatten()\n",
    "\n",
    "# NTM\n",
    "class NTMModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(NTMModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(20, activation=\"tanh\")\n",
    "        self.dense2 = tf.keras.layers.Dense(20, activation=\"tanh\")\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Train\n",
    "t_data, x_data, u_data = generate_data(100, 1, 2*np.pi)\n",
    "train_data = np.stack([t_data, x_data], axis=1)\n",
    "\n",
    "\n",
    "model = NTMModel()\n",
    "\n",
    "\n",
    "def train_step(model, inputs, outputs, optimizer):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(inputs)\n",
    "        predicted = model(inputs)\n",
    "        loss = tf.reduce_mean((predicted - outputs)**2)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.01)\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    inputs = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "    outputs = tf.convert_to_tensor(u_data.reshape(-1, 1), dtype=tf.float32)\n",
    "    loss = train_step(model, inputs, outputs, optimizer)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Evaluate the discovered PDE\n",
    "def evaluate_pde(model, t, x):\n",
    "    X = tf.stack([t, x], axis=1)\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch(X)\n",
    "        U = model(X)\n",
    "        U_t = tape1.gradient(U, t)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch(X)\n",
    "        U_x = tape2.gradient(U, x)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape3:\n",
    "        tape3.watch(X)\n",
    "        U_xx = tape3.gradient(U_x, x)\n",
    "    \n",
    "    return U_t, U_xx\n",
    "\n",
    "# Check the PDE residuals\n",
    "t_test = tf.convert_to_tensor(t_data, dtype=tf.float32)\n",
    "x_test = tf.convert_to_tensor(x_data, dtype=tf.float32)\n",
    "U_t, U_xx = evaluate_pde(model, t_test, x_test)\n",
    "pde_residuals = U_t - U_xx\n",
    "print(\"PDE Residuals Mean:\", pde_residuals.numpy().mean())\n",
    "print(\"PDE Residuals Std:\", pde_residuals.numpy().std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7903d9",
   "metadata": {},
   "source": [
    "We notice that the loss $4.7597408411093056e-05$ beats the PEQL with same epochs. It shows that the NTM is more efficient in this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f35ba",
   "metadata": {},
   "source": [
    "### Although we don't find any new PDE equations using only simple heat equation $u_t=u_{xx}$, the idea of NTM is theoretically reasonable and able to find the potential new PDE from give data. It worth some time to do further research work on the coding realization on other PDE discovering using NTM. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
